1. JKI is de snelste van de eerste 6 oplossingen, KJI komt zeer dicht in de buurt.
I wordt het best als binnenste loop uitgevoerd omdat deze altijd in de zelfde kollom blijven.

--> c(i,j) = c(i,j) + a(i,k)*b(k,j)

Merk op dat i bij c en a itereerd door de  kollom met index j of k.
In b zal k itereren door de jde kollom

Dit is in in Fortran sneller omdat de matrixen kollom per kollom worden bijgehouden. 
Als een element uit de matrix wordt opgehaald zullen de volgende elementen uit de kollom 
gecached worden. Als dit net de elementen zijn die nodig zijn in de volgende iteratie zal 
het algortime dus een stuk sneller zijn.

2. De vector operaties zijn duidelijk sneller dan de individueele operaties.
De compiler kan hier dus beter optimaliseren.

3. Het dot product wordt uitgerekend tussen  de rijen van a en de kollommen van b. 
Het uitlezen van de rijen van a zal traag gebeuren omdat opnieuw fortran de matrixen per kollom bewaard.

4. Door a te transponeren wordt het dot product berekend tussen twee kollommen, Het uitlezen
van de kollommen van een matrix kan zeer snel gebeuren. Er is duidelijk een tijdwinst.


5. BLAS lijkt raar genoeg geen meerdere cores te gebruiken, eerst keek op ik top als er meerdere 
core's belast waren. Dit bleek niet zo te zijn, dus vroeg ik me af of er wel nog verschillende processen waren. 
In principe kan het zijn dat de scheduler alle taken op 1 core plaatst, ps ax | grep a.out gaf aan dat er maar 
1 proces was.


6. wat is de L2 cach grote? volgens /proc/cpu is de L2 cache 2KB groot 
Dit betekend dat als we 2 (deel)matrixen hebben van groote n*n (a,b) en die in de cache willen krijgen 
we dus een blokgrootte nemen van ongeveer:
    sqrt(2048000/32)/2 = 125
Dit is alsook waar te nemen met een klein experiment:
We nemen een matrix van groote N=2048 en varieeren de blok grote n=?

n=32 16.03 seconden
n=64 13.89 seconden
n=128 13.46 seconden
n=256 17.28 seconden
n=512 15.49 seconden
n=1024 15.01 seconden

Als we de grote zo kiezen dat de matrices volledig in het ram kunnen, zal de methode van het
vermenigivuldigen geen merkwaardig verschil meer maken voor de snelheid.

7. matmul
We doen opnieuw hetzelfde experiment met een matrix van N=2048, dit maal varieeren we de blocksize
van matmul bij het compileren

met gfortran:

n=16 15.1 seconden
n=32 14.99 seconden
n=64 15.08 seconden
n=128 15.08 seconden
n=256 15.06 seconden
n=512 15.03 seconden

nagfor is ZEEER traag 37 seconden voor N=1024, onafhankelijk van de blocksize erbij of niet
