1. JKI is de snelste van de eerste 6 oplossingen, KJI komt zeer dicht in de buurt.
I wordt het best als binnenste loop uitgevoerd omdat deze altijd in de zelfde kollom blijven.

--> c(i,j) = c(i,j) + a(i,k)*b(k,j)

Merk op dat i bij c en a itereerd door de  kollom met index j of k.
In b zal k itereren door de jde kollom

Dit is in in Fortran sneller omdat de matrixen kollom per kollom worden bijgehouden. 
Als een element uit de matrix wordt opgehaald zullen de volgende elementen uit de kollom 
gecached worden. Als dit net de elementen zijn die nodig zijn in de volgende iteratie zal 
het algortime dus een stuk sneller zijn.

2. De vector operaties zijn duidelijk sneller dan de individueele operaties.
De compiler kan hier dus beter optimaliseren.

3. Het dot product wordt uitgerekend tussen  de rijen van a en de kollommen van b. 
Het uitlezen van de rijen van a zal traag gebeuren omdat opnieuw fortran de matrixen per kollom bewaard.

4. Door a te transponeren wordt het dot product berekend tussen twee kollommen, Het uitlezen
van de kollommen van een matrix kan zeer snel gebeuren. Er is duidelijk een tijdwinst.


5. BLAS lijkt raar genoeg geen meerdere cores te gebruiken, eerst keek op ik top als er meerdere 
core's belast waren. Dit bleek niet zo te zijn, dus vroeg ik me af of er wel nog verschillende processen waren. 
In principe kan het zijn dat de scheduler alle taken op 1 core plaatst, ps ax | grep a.out gaf aan dat er maar 
1 proces was.


6. wat is de L2 cach grote? volgens /proc/cpu is de cache 256K groot
De formule uit de presenties van de voorbereiding vermeld volgende formule:
    de blocksize moet dus kleiner zijn dan sqrt(M/3)
Aangezien gewerkt wordt met double precision die 8 bytes per woord bevatten moeten we M ook nog delen
door 8 als we de eenheid van n willen idpv het aantal bytes.

-> max_blocksize = sqrt((M/3)/aantal bytes in 1 getal) = sqrt(((256000/8)/3)) = 103.2796


Dit is alsook waar te nemen met een klein experiment:
We nemen een matrix van groote N=3000 en varieeren de blok grote n=?

n=50   | 8.88  seconden
n=100  | 8.14  seconden
n=120  | 7.99  seconden
n=150  | 9.46  seconden
n=300  | 8.35  seconden
n=1000 | 12.18 seconden

Als we de grote zo kiezen dat de deel matrices volledig in L2 cache kunnen, zal de methode van het
vermenigivuldigen geen merkwaardig verschil meer maken voor de snelheid.

Hetzelfde met nagfor leverd een veel slechter resultaat op. Een matrix van 2000*2000 elementen
en een bocksize van 100 nemen 23 seconden in beslag. ( nagfor -O3 dmr.f90 matrixop.o -lblas)

7. matmul
We doen opnieuw hetzelfde experiment met een matrix van N=2048, dit maal varieeren we de blocksize
van matmul bij het compileren


gfortran -c matrixop.o  -lblas -fopenmp --param loop-block-tile-size=100
gfortran dmr.f90 matrixop.o -lblas -fopenmp --param loop-block-tile-size=100
nagfor: N=3000
---------------------
n=500 | 10.17 seconden
n=200 | 9.98
n=100 | 8.86 seconden
n=50  | 9.3 seconden


gfortran -c matrixop.f90 -O3 -lblas -floop-block -fopenmp --param loop-block-tile-size=500
gfortran -O3 dmr.f90 matrixop.o -lblas -floop-block -fopenmp --param loop-block-tile-size=500
gfortran: N=3000
---------------------
n=500 | 13.73 seconden
n=200 | 13.67 seconden
n=100 | 13.58 seconden
n=50  | 13.67 seconden


